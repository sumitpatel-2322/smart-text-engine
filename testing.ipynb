{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "970bb250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "58c9c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c8dc7e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "98f73db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /distilbert-base-uncased/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"HTTPSConnection(host=\\'huggingface.co\\', port=443): Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 21076054-008c-4a2a-b3c5-10a3bb306a71)')' thrown while requesting HEAD https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /distilbert-base-uncased/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"HTTPSConnection(host=\\'huggingface.co\\', port=443): Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 52b5f866-08fb-4ba4-b0ae-436e4b960593)')' thrown while requesting HEAD https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 2s [Retry 2/5].\n"
     ]
    }
   ],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6e719643",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[\"I am Learning transformers by practising it Nice move right??.\",\n",
    "      \"This is the second line to test what happens.\",\n",
    "      \"This is the third line again, for the same purpose.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "44f84033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(3, 17), dtype=int32, numpy=\n",
      "array([[  101,  1045,  2572,  4083, 19081,  2011, 10975, 18908,  9355,\n",
      "         2009,  3835,  2693,  2157,  1029,  1029,  1012,   102],\n",
      "       [  101,  2023,  2003,  1996,  2117,  2240,  2000,  3231,  2054,\n",
      "         6433,  1012,   102,     0,     0,     0,     0,     0],\n",
      "       [  101,  2023,  2003,  1996,  2353,  2240,  2153,  1010,  2005,\n",
      "         1996,  2168,  3800,  1012,   102,     0,     0,     0]])>, 'attention_mask': <tf.Tensor: shape=(3, 17), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])>}\n"
     ]
    }
   ],
   "source": [
    "tokens=tokenizer(text,\n",
    "                 padding=True,\n",
    "                 truncation=True,\n",
    "                 return_tensors='tf')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e67f6aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids shape: (3, 17)\n",
      "Input atention mask shape: (3, 17)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input ids shape: {tokens['input_ids'].shape}\")\n",
    "print(f\"Input atention mask shape: {tokens['attention_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f448c3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input IDs:\n",
      "[[  101  1045  2572  4083 19081  2011 10975 18908  9355  2009  3835  2693\n",
      "   2157  1029  1029  1012   102]\n",
      " [  101  2023  2003  1996  2117  2240  2000  3231  2054  6433  1012   102\n",
      "      0     0     0     0     0]\n",
      " [  101  2023  2003  1996  2353  2240  2153  1010  2005  1996  2168  3800\n",
      "   1012   102     0     0     0]]\n",
      "\n",
      "Input Attention masks:\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nInput IDs:\\n{tokens['input_ids']}\")\n",
    "print(f\"\\nInput Attention masks:\\n{tokens['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "52de3ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_projector', 'vocab_transform', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model=TFAutoModel.from_pretrained(\"distilbert-base-uncased\",use_safetensors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cff76768",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=model(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f3a1edb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFBaseModelOutput(last_hidden_state=<tf.Tensor: shape=(3, 17, 768), dtype=float32, numpy=\n",
      "array([[[ 2.20871031e-01,  7.54742250e-02,  1.19936213e-01, ...,\n",
      "         -8.43691751e-02,  3.36341649e-01,  3.63004923e-01],\n",
      "        [ 6.01343334e-01,  2.87427288e-02, -8.81800801e-02, ...,\n",
      "         -1.95379809e-01,  4.72915173e-01,  3.71741027e-01],\n",
      "        [ 3.18101972e-01,  4.56485957e-01,  1.12842649e-01, ...,\n",
      "         -2.08585277e-01,  3.12930703e-01,  4.15247381e-01],\n",
      "        ...,\n",
      "        [ 2.96755433e-01, -2.92393148e-01,  1.04695596e-01, ...,\n",
      "         -2.80010641e-01,  9.78622809e-02,  2.31774926e-01],\n",
      "        [ 2.77177803e-02, -6.23123825e-01,  2.75297284e-01, ...,\n",
      "          8.59816819e-02,  1.47867173e-01, -2.89324880e-01],\n",
      "        [ 9.73046958e-01,  4.23277408e-01, -3.60569656e-01, ...,\n",
      "          1.05962306e-01, -4.64780480e-01, -3.32707494e-01]],\n",
      "\n",
      "       [[-1.48288265e-01, -1.95132419e-01,  9.69076306e-02, ...,\n",
      "         -1.04272347e-02,  2.87950218e-01,  4.10171092e-01],\n",
      "        [-4.05340850e-01, -4.75430489e-01,  6.51665032e-03, ...,\n",
      "         -1.13020331e-01,  3.95253301e-01,  4.15507138e-01],\n",
      "        [-6.73608661e-01, -3.39851499e-01,  2.49581367e-01, ...,\n",
      "          1.18393660e-01,  9.38802958e-03,  8.08112621e-01],\n",
      "        ...,\n",
      "        [-2.24033762e-02, -1.97142527e-01,  2.79796660e-01, ...,\n",
      "          8.11124444e-02, -6.77329451e-02,  2.03806385e-01],\n",
      "        [-9.37529206e-02, -3.07967484e-01,  2.59934366e-01, ...,\n",
      "          2.18982771e-01, -1.06271334e-01,  2.08263338e-01],\n",
      "        [-1.27805099e-01, -3.34881216e-01,  2.02075064e-01, ...,\n",
      "          2.07612038e-01, -9.21929330e-02,  1.93228170e-01]],\n",
      "\n",
      "       [[-1.66518822e-01, -1.21550247e-01,  1.61409602e-02, ...,\n",
      "          9.74180400e-02,  3.03219944e-01,  4.13383991e-01],\n",
      "        [-3.25611681e-01, -2.95053929e-01, -8.96941274e-02, ...,\n",
      "         -1.08506277e-01,  4.06276137e-01,  2.60272861e-01],\n",
      "        [-4.67066765e-01, -1.44627497e-01,  1.25530779e-01, ...,\n",
      "         -2.17143744e-02, -9.74640623e-03,  7.10884392e-01],\n",
      "        ...,\n",
      "        [-1.80649564e-01, -2.91577019e-02,  2.39141151e-01, ...,\n",
      "          1.49187475e-01, -3.75054777e-04,  2.43961886e-01],\n",
      "        [-2.04065233e-01, -1.04550809e-01,  2.33438939e-01, ...,\n",
      "          1.85131952e-01,  3.58435735e-02,  2.68068343e-01],\n",
      "        [-1.46866694e-01, -9.68181044e-02,  1.94000959e-01, ...,\n",
      "          1.11211754e-01,  2.88352054e-02,  2.24314988e-01]]],\n",
      "      dtype=float32)>, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4facf546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Last Hidden state shape: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 17, 768])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nLast Hidden state shape: \\n\")\n",
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "de49a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings=outputs.last_hidden_state \n",
    "attention_mask=tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "96afafdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=tf.cast(attention_mask,tf.float32)\n",
    "mask=tf.expand_dims(mask,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cfc4393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_embeddings=token_embeddings*mask\n",
    "sum_embeddings=tf.reduce_sum(masked_embeddings,axis=1)\n",
    "sum_mask=tf.reduce_sum(mask,axis=1)\n",
    "sentence_embeddings=sum_embeddings/sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9c10f0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Embeddings shape: (3, 768)\n",
      "tf.Tensor(\n",
      "[[ 0.41552278  0.05296225  0.1476448  ... -0.18735738  0.06943531\n",
      "   0.13740087]\n",
      " [-0.12765019 -0.34959766  0.11867136 ...  0.05103514  0.0351815\n",
      "   0.10801701]\n",
      " [-0.09582864 -0.10354149  0.10216471 ... -0.00803287  0.05200306\n",
      "   0.21631825]], shape=(3, 768), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentence Embeddings shape: {sentence_embeddings.shape}\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1953de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "classifier=layers.Dense(\n",
    "    units=2,\n",
    "    activation='softmax'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1bfef12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_logits=classifier(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7b8c2661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Output shape: (3, 2)\n",
      "tf.Tensor(\n",
      "[[0.38451484 0.61548513]\n",
      " [0.3941242  0.60587573]\n",
      " [0.55034655 0.44965342]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentiment Output shape: {sentiment_logits.shape}\")\n",
    "print(sentiment_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9c4323e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Probabilities shape: (3, 3)\n",
      "tf.Tensor(\n",
      "[[0.33221337 0.41658875 0.2511978 ]\n",
      " [0.42829114 0.30165958 0.2700493 ]\n",
      " [0.33368778 0.4141574  0.25215483]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "num_classes=3\n",
    "classifier=tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(\n",
    "        num_classes,\n",
    "        activation='softmax'\n",
    "    )\n",
    "])\n",
    "\n",
    "sentiment_probs=classifier(sentence_embeddings)\n",
    "print(f\"Sentiment Probabilities shape: {sentiment_probs.shape}\")\n",
    "print(sentiment_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e9d6d48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss value: 1.2258400917053223\n"
     ]
    }
   ],
   "source": [
    "labels=tf.constant([2,1,0])\n",
    "loss_fn=tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "loss_value=loss_fn(labels,sentiment_probs)\n",
    "print(f\"Loss value: {loss_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d76d60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "with tf.GradientTape() as tape:\n",
    "    predictions=classifier(sentence_embeddings)\n",
    "    loss=loss_fn(labels,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2a31361f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2258401"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "49dd25b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients=tape.gradient(loss,classifier.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b49aa17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.apply_gradients(zip(gradients,classifier.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0a87f3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after one training step: 1.2258400917053223\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loss after one training step: {loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d7c38e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 Loss: 0.6372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step2 Loss: 0.2988\n",
      "Step3 Loss: 0.1440\n",
      "Step4 Loss: 0.0689\n",
      "Step5 Loss: 0.0354\n",
      "Step6 Loss: 0.0199\n",
      "Step7 Loss: 0.0121\n",
      "Step8 Loss: 0.0077\n",
      "Step9 Loss: 0.0052\n",
      "Step10 Loss: 0.0036\n",
      "Step11 Loss: 0.0025\n",
      "Step12 Loss: 0.0019\n",
      "Step13 Loss: 0.0014\n",
      "Step14 Loss: 0.0011\n",
      "Step15 Loss: 0.0009\n",
      "Step16 Loss: 0.0007\n",
      "Step17 Loss: 0.0006\n",
      "Step18 Loss: 0.0005\n",
      "Step19 Loss: 0.0004\n",
      "Step20 Loss: 0.0004\n"
     ]
    }
   ],
   "source": [
    "for step in range(20):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions=classifier(sentence_embeddings)\n",
    "        loss=loss_fn(labels,predictions)\n",
    "    gradients=tape.gradient(loss,classifier.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients,classifier.trainable_variables))\n",
    "    print(f\"Step{step+1} Loss: {loss.numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c351cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class indices: [2 1 0]\n",
      "\n",
      "Sentence: I am Learning transformers by practising it Nice move right??.\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "Sentence: This is the second line to test what happens.\n",
      "Predicted Sentiment: Neutral\n",
      "\n",
      "Sentence: This is the third line again, for the same purpose.\n",
      "Predicted Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "final_probs = classifier(sentence_embeddings)\n",
    "\n",
    "predicted_classes = tf.argmax(final_probs, axis=1)\n",
    "\n",
    "print(\"Predicted class indices:\", predicted_classes.numpy())\n",
    "\n",
    "label_map = {\n",
    "    0: \"Negative\",\n",
    "    1: \"Neutral\",\n",
    "    2: \"Positive\"\n",
    "}\n",
    "\n",
    "for i, sentence in enumerate(text):\n",
    "    sentiment = label_map[int(predicted_classes[i])]\n",
    "    print(f\"\\nSentence: {sentence}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0dd96b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
